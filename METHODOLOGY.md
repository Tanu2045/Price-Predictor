# Methodology Overview

## Objectives
- Predict product prices using the provided text, image, and tabular signals while preserving the original dataset layout inside `../test1`.
- Prioritise a DistilBERT-based regressor for the initial iteration; stage hooks for future vision (ViT/EfficientNet) and XGBoost models.
- Run all heavy training/inference workloads on the remote GPU server accessed via SSH.

## Environment Setup
1. SSH into the GPU machine and activate the challenge environment:
   ```bash
   conda activate d2l
   ```
2. Inside `/home/pranav/projects/az_ml_challenge/master`, initialise (one time):
   ```bash
   uv init --package .
   uv add torch transformers accelerate datasets pandas numpy scikit-learn pyyaml tqdm
   ```
3. Confirm GPU visibility for PyTorch:
   ```bash
   uv run python -c "import torch; print(torch.cuda.is_available())"
   ```

## Data Management
- Keep raw artefacts (`dataset/`, `images/`, `catalog.db`) in `/home/pranav/projects/az_ml_challenge/test1`.
- Access them via the configuration file `config/paths.yml`; paths are resolved relative to the project root, with `root_dir: ../test1` ensuring correct lookups.
- Optional: create read-only symlinks (e.g., `ln -s ../test1/dataset dataset_link`) if local tooling requires canonical in-project paths, but do not move files.

## BERT Training Pipeline
1. **Configuration**: Adjust hyperparameters in `config/paths.yml` under `model.bert`. Key knobs include `max_length`, `batch_size`, and `learning_rate`.
2. **Training**:
   ```bash
   uv run python scripts/train_bert.py --config config/paths.yml --mixed_precision bf16
   ```
   - Loads train/validation splits, tokenises text on the fly, and fits a regression head on DistilBERT using `accelerate`.
   - Checkpoints are stored under `checkpoints/bert/best/` alongside `best_metrics.json`.
3. **Evaluation** (re-runs validation against the saved split):
   ```bash
   uv run python scripts/eval_bert.py --config config/paths.yml --checkpoint checkpoints/bert/best
   ```
4. **Inference**:
   ```bash
   uv run python scripts/predict_bert.py --config config/paths.yml --checkpoint checkpoints/bert/best --output outputs/predictions/bert.csv
   ```
   - Produces competition-formatted predictions and metadata describing the run.

## Ensemble Strategy
- Maintain individual prediction CSVs for each model in `outputs/predictions/`.
- Average them when additional models are ready:
  ```bash
  uv run python -c "from master.ensemble import average_prediction_files; average_prediction_files(['outputs/predictions/bert.csv', 'outputs/predictions/vision.csv', 'outputs/predictions/xgboost.csv'], 'outputs/predictions/ensemble.csv')"
  ```
- Only include models with validated SMAPE scores to avoid degrading the ensemble.

## Future Work Hooks
- **Vision Models**: `models/hf/` contains placeholders for ViT/EfficientNet checkpoints. Implement dedicated scripts later (`train_vision.py`, etc.) mirroring the BERT structure.
- **Regex/Parsing Features**: `placeholder_feature_harness` in `master.data.dataset` allows plugging in NumPy feature blocks generated by forthcoming parsing scripts; these features will feed both BERT (as auxiliary inputs) and XGBoost.

## Manual Intervention Checklist
- Confirm Hugging Face downloads (`distilbert-base-uncased`, future ViT/EfficientNet) succeed and have enough disk space; snapshot into `models/hf/` if offline access is required.
- Monitor training runs via logged SMAPE/MAE metrics (`outputs/logs/bert_training_history.json`); tweak hyperparameters or learning schedules as needed.
- Before ensembling, review prediction distributions from each model to ensure compatibility for averaging (no severe biases or negative outputs).
- Keep track of regex feature extraction progress and regenerate cached feature matrices when parsers are updated.
